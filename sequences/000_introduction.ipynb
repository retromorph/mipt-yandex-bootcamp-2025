{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d215a593-9d0b-4339-9cf5-c3325fb52f3d",
   "metadata": {},
   "source": [
    "# Введение в нейросети для обработки последовательностей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd14c149-e3f3-4028-b176-7dadb8671d2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Примеры задач"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d6e4e-1f99-4551-b255-4298e45c911b",
   "metadata": {},
   "source": [
    "Подходящие форматы входных данных:\n",
    "- **Текст**\n",
    "- Музыка\n",
    "- Видео\n",
    "- Временные ряды\n",
    "- ДНК\n",
    "- Короче все что можно представить в виде последовательности, если немного потанцевать с бубном.\n",
    "\n",
    "Формальнее: пусть $X \\subset \\mathbb{R}^n$ - пространство входных данных. Если мы можем выделить разумное направление из n доступных и дискретизировать элементы $X$ по нему, то можно юзать нейросети для обработки последовательностей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe833cd-b702-47b9-ba3e-d5e77b56221d",
   "metadata": {},
   "source": [
    "По большей части будем рассматривать тексты, т.е. заниматься NLP.\n",
    "\n",
    "Примеры задач NLP:\n",
    "- классификация документов (по темам, рубрикам, жанрам и так далее)\n",
    "- определение спама\n",
    "- определение частей речи\n",
    "- исправление орфографических ошибок и опечаток\n",
    "- поиск ключевых слов, синонимов / антонимов в тексте\n",
    "- распознавание именованных сущностей (имен, названий географических объектов, дат, номеров телефонов, адресов)\n",
    "- определение эмоциональной окраски текста (sentiment analysis)\n",
    "- поиск релевантных документов по запросу, а также их ранжирование\n",
    "- задача суммаризации (автоматическое составление краткого пересказа текста)\n",
    "- автоматический перевод с одного языка на другой (машинный перевод)\n",
    "- диалоговые системы и чат-боты\n",
    "- вопросно-ответные системы (выбор ответа из нескольких предложенных вариантов или вопросы с открытым ответом)\n",
    "- кроме того, к NLP также относят задачу распознавания речи (Automated Speech Recognition, ASR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a1d018-f532-4385-9102-10dc4cc507c3",
   "metadata": {},
   "source": [
    "## Определения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e301db1-8bd1-45fa-b609-3fa2b2511717",
   "metadata": {},
   "source": [
    "Пусть $X \\subset \\mathbb{R}^k$ - пространство входных символов (например эмбеддинги слов), а $Y \\subset \\mathbb{R}^l$ - пространство выходных символов. Обозначим за $X_{seq}$ пространство всевозможных конечных последовательностей элементов из $X$. Аналогично введем $Y_{seq}$\n",
    "\n",
    "**Определение:**\n",
    "- Many-to-one модель для работы с последовательностя - отображение $F: X_{seq} \\longrightarrow Y$\n",
    "- One-to-many модель для работы с последовательностя - отображение $F: X \\longrightarrow Y_{seq}$\n",
    "- Несинхронизированная many-to-many модель для работы с последовательностя - отображение $F: X_{seq} \\longrightarrow Y_{seq}$\n",
    "- Синхронизированная many-to-many модель для работы с последовательностями - как несинхронизированная, но с условием что $\\forall x \\in X_{seq}$  $|x| = |F(x)|$\n",
    "\n",
    "**Примеры:**\n",
    "\n",
    "<img src=\"../media/sequence_models_example.png\">\n",
    "\n",
    "Разберемся сначала с более простыми Many-to-one и Sync many-to-many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f35e6-6454-494a-ac85-3d3a015f0563",
   "metadata": {},
   "source": [
    "## Эмбеддинги\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f6446-c5ba-4548-a7a9-44811367da74",
   "metadata": {},
   "source": [
    "Для того, чтобы работать с данными с помощью нейросетей для обработки последовательностей нужно собственно уметь превращать элементы входной/выходной последовательности в элементы $X$/$Y$. $X$/$Y$ в таком случае называются пространствами эмбеддингов.\n",
    "Возможны так же варианты, когда вся входная/выходная последовательность целиком укладывается в один вектор уже в $X_{seq}$/$Y_{seq}$. Тогда уже $X_{seq}$/$Y_{seq}$ можно назвать пространствами эмбеддингов.\n",
    "\n",
    "Разберем все это на примере NLP:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5367b90b-f085-488c-bfe9-04e8520017db",
   "metadata": {},
   "source": [
    "### Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d03204-11af-4709-9799-ddb264ae9cc2",
   "metadata": {},
   "source": [
    "Пусть $T$ - пространство всевозможных текстов на английском языке. Построим эмбеддинг $E: T \\longrightarrow X_{seq}$ таким образом:\n",
    "- Выберем словарь известных слов\n",
    "- Удалим из него наиболее часто встречающиеся и не несущие особого смысла слова (the, a, an, I, etc.)\n",
    "- Таким образом у нас получится $D = {d_1, d_2, ..., d_n}$\n",
    "- Переводим текст $t$ в вектор $(n_{d_1}, n_{d_2}, ..., n_{d_n})$, где $n_{d_i}$ - количество вхождений слова $d_i$ в тексте $t$\n",
    "\n",
    "Возможны так же различные трюки, вроде того, что мы формируем словарь из N-грам, т.е. последовательностей из N слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60d206da-f667-47e8-b85c-655b36ca2f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.str_('william'): 1,\n",
       " np.str_('shakespeare'): 2,\n",
       " np.str_('an'): 1,\n",
       " np.str_('english'): 3,\n",
       " np.str_('poet'): 2,\n",
       " np.str_('he'): 2,\n",
       " np.str_('widely'): 1,\n",
       " np.str_('regarded'): 1,\n",
       " np.str_('greatest'): 1,\n",
       " np.str_('writer'): 2,\n",
       " np.str_('language'): 2,\n",
       " np.str_(\"world's\"): 1,\n",
       " np.str_('often'): 2,\n",
       " np.str_('called'): 1,\n",
       " np.str_('national'): 1,\n",
       " np.str_('or'): 1,\n",
       " np.str_('simply'): 1,\n",
       " np.str_('his'): 3,\n",
       " np.str_('including'): 1,\n",
       " np.str_('consist'): 1,\n",
       " np.str_('some'): 2,\n",
       " np.str_('three'): 1,\n",
       " np.str_('long'): 1,\n",
       " np.str_('narrative'): 1,\n",
       " np.str_('poems'): 1,\n",
       " np.str_('few'): 1,\n",
       " np.str_('other'): 2,\n",
       " np.str_('uncertain'): 1,\n",
       " np.str_('plays'): 1,\n",
       " np.str_('been'): 1,\n",
       " np.str_('translated'): 1,\n",
       " np.str_('into'): 1,\n",
       " np.str_('every'): 1,\n",
       " np.str_('major'): 1,\n",
       " np.str_('living'): 1,\n",
       " np.str_('performed'): 1,\n",
       " np.str_('more'): 1,\n",
       " np.str_('than'): 1,\n",
       " np.str_('those'): 1,\n",
       " np.str_('any'): 1,\n",
       " np.str_('remains'): 1,\n",
       " np.str_('arguably'): 1,\n",
       " np.str_('most'): 1,\n",
       " np.str_('influential'): 1,\n",
       " np.str_('works'): 1,\n",
       " np.str_('continue'): 1,\n",
       " np.str_('studied'): 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ahocorasick\n",
    "from wordfreq import top_n_list\n",
    "from typing import List\n",
    "\n",
    "class BoW:\n",
    "    def __init__(self, dictionary: List[str]):\n",
    "        self.dictionary = np.array(dictionary)\n",
    "        self.dictionary_fsm = ahocorasick.Automaton()\n",
    "        \n",
    "        for idx, key in enumerate(self.dictionary):\n",
    "            self.dictionary_fsm.add_word(key, (idx, key))\n",
    "        self.dictionary_fsm.make_automaton()\n",
    "\n",
    "    def transform(self, text: str):\n",
    "        text = text.lower()\n",
    "        embedding = np.zeros(self.dictionary.shape[0])\n",
    "        \n",
    "        for  end_index, (idx, _) in self.dictionary_fsm.iter(text):\n",
    "            start_index = end_index - len(word) + 1\n",
    "            \n",
    "            if (start_index == 0 or text[start_index - 1] == ' ') and (len(text) == end_index + 1 or text[end_index + 1] == ' '):\n",
    "                embedding[idx] += 1\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "    def transform_readable(self, text: str):\n",
    "        text = text.lower()\n",
    "        result = {}\n",
    "        \n",
    "        for  end_index, (_, word) in self.dictionary_fsm.iter(text):\n",
    "            start_index = end_index - len(word) + 1\n",
    "            \n",
    "            if (start_index == 0 or text[start_index - 1] == ' ') and (len(text) == end_index + 1 or text[end_index + 1] == ' '):\n",
    "                if word not in result:\n",
    "                    result[word] = 1\n",
    "                else:\n",
    "                    result[word] += 1\n",
    "        \n",
    "        return result\n",
    "        \n",
    "dictionary = top_n_list(\"en\", n=10000)[20:]\n",
    "bow = BoW(dictionary)\n",
    "bow.transform_readable(\"William Shakespeare was an English playwright, poet and actor. He is widely regarded as the greatest writer in the English language and the world's pre-eminent dramatist. He is often called England's national poet and the \\\"Bard of Avon\\\" or simply \\\"the Bard\\\". His extant works, including collaborations, consist of some 39 plays, 154 sonnets, three long narrative poems and a few other verses, some of uncertain authorship. His plays have been translated into every major living language and are performed more often than those of any other playwright. Shakespeare remains arguably the most influential writer in the English language, and his works continue to be studied and reinterpreted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5071b729-8550-4744-b136-fceb81b67607",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5990f573-5b29-4571-a8d2-eeae40e591b5",
   "metadata": {},
   "source": [
    "Главная проблема bag-of-words в том, что он никак не учитывает контекст в котором мы работает. Эту проблему частично решает TF-IDF. Пусть у нас есть коллекция документов $C = {F_1, F_2, ..., F_k}$ - наш контекст.\n",
    "\n",
    "Идея аналогичная BoW, но теперь в итоговом векторе эмбединга на $i$-том месте стоит не просто число вхождений слова в текст, а велечина $TF(d_i, t) \\cdot IDF(d_i, C)$, где\n",
    "- $TF(d_i, t) = \\frac{n_{d_i}}{|t|}$ - частота вхождения $d_i$ в текст (term frquency)\n",
    "- $IDF(d_i, C) = \\log(\\frac{|C|}{|C_{d_i}|})$, где $|C_{d_i}|$ - количество документов из нашего контекста, которые содержат слово $d_i$. Этот множитель штрафует компоненты, отвечающие слишком распространённым токенам, и повышает вес специфических для отдельных текстов (и, вероятно, информативных) слов. (inversed document frequiency)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T00:31:31.546643Z",
     "start_time": "2025-04-30T00:31:31.535219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TFIDF:\n",
    "    pass"
   ],
   "id": "61993dd716292ebe",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "968ea3d2-920a-4f42-b869-28b873070edc",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2300d5d7-10d7-420a-a268-c9e0b8166af2",
   "metadata": {},
   "source": [
    "Word2Vec развивает идею работы с контекстом. Де-факто это попытка построить эмбеддинг слова исходя из контекста его употребления, что логично и обосновано с точки зрения лингвистики.\n",
    "\n",
    "Рассмотрим один из вариантов построения w2v - CBoW (Continuous bag-of-words).\n",
    "Зафиксируем гиперпараметры:\n",
    "- Словарь $D = {d_1, d_2, ..., d_n}$\n",
    "- Размер контекста $l$ (например, если 1, то контекстом слова считаются одно слово до него и одно после)\n",
    "- Размерность пространства эмбеддингов $k$\n",
    "\n",
    "Для каждого слова $t$ заведем векторы $v_t, \\omega_t \\in \\mathbb{R}^k$. $v_t$ - эмбеддинг слова, когда оно является центральным. $\\omega_t$ - эмбеддинг слова, когда оно является частью контекста.\n",
    "\n",
    "Таким образом у нас $2 \\cdot n \\cdot k$ параметров в модели.\n",
    "\n",
    "На вход подается $C = {c_1, ..., c_{2*l}}$ - контекст ($l$ слов слева от центрального и столько же справа). Для каждого слова из словаря мы вычисляем $logits_d = (\\sum_{i = 1}^{2*l} \\omega_{c_i}, v_d)$. Затем с помощью $softmax(logits)$ вычисляем предсказанное распределение центральных слов.\n",
    "\n",
    "В конце-концов считаем loss с помощью кросс-энтропии с истинным распределением и обучаем модель с помощью SGD\n",
    "\n",
    "<img src=\"../media/CBOW.png\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3841ab-6e48-426b-9fab-fe4ea807f559",
   "metadata": {},
   "source": [
    "## Рекуррентные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250f487-0897-46aa-9ad4-75f336970deb",
   "metadata": {},
   "source": "<img src=\"../media/RNN.png\">"
  },
  {
   "cell_type": "markdown",
   "id": "c1c63815-3726-4ad6-8cec-5c6cac9fcd3e",
   "metadata": {},
   "source": [
    "### Пример использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379b6fb4-69c8-49a8-8a38-923326320ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec9bf7f-7c83-4e33-b8cd-fec566a27462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirQualityRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, nonlinearity='tanh')\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)  # (batch_size, seq_len, hidden_size)\n",
    "        out = out[:, -1, :]   # Take last output\n",
    "        return self.linear(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2572672d-d533-4a5e-b09d-07ad0047c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup for RNN\n",
    "rnn_model = AirQualityRNN(input_size=train_data.shape[1], \n",
    "                         hidden_size=128, \n",
    "                         num_layers=2).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for RNN\n",
    "print(\"\\nTraining Standard RNN:\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    rnn_model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        rnn_optimizer.zero_grad()\n",
    "        outputs = rnn_model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        rnn_optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    rnn_model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            y_pred = rnn_model(X_test)\n",
    "            test_loss += criterion(y_pred, y_test).item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1:2} | Train Loss: {loss.item():.4f} | Test Loss: {test_loss/len(test_loader):.4f}')\n",
    "\n",
    "# RNN Evaluation\n",
    "rnn_model.eval()\n",
    "rnn_predictions, rnn_actuals = [], []\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in test_loader:\n",
    "        X_test = X_test.to(device)\n",
    "        y_pred = rnn_model(X_test).cpu().numpy().flatten()\n",
    "        rnn_predictions.extend(y_pred)\n",
    "        rnn_actuals.extend(y_test.numpy().flatten())\n",
    "\n",
    "# Inverse scaling for RNN\n",
    "rnn_predictions = np.array(rnn_predictions) * scaler.scale_[CO_GT_INDEX] + scaler.mean_[CO_GT_INDEX]\n",
    "rnn_actuals = np.array(rnn_actuals) * scaler.scale_[CO_GT_INDEX] + scaler.mean_[CO_GT_INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d4217-6d90-43e8-bba9-a40d9db08a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE for RNN\n",
    "rnn_rmse = np.sqrt(mean_squared_error(rnn_actuals, rnn_predictions))\n",
    "print(f'\\nRNN Final RMSE: {rnn_rmse:.2f}')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LSTM",
   "id": "c153830ed89dc54f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GRU",
   "id": "bda4c826a59d9ec4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Attention",
   "id": "f4f10f71ac419786"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Self-attention",
   "id": "5dffcd68d62f6206"
  },
  {
   "cell_type": "markdown",
   "id": "1752cecb-4b65-4c06-b250-ea34775e8bbe",
   "metadata": {},
   "source": [
    "## Источники"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a6baa2-eeaa-4fda-8a61-ad1ae773560c",
   "metadata": {},
   "source": [
    "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami\n",
    "\n",
    "https://lena-voita.github.io/nlp_course/word_embeddings.html\n",
    "\n",
    "Word2Vec - https://arxiv.org/abs/1301.3781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502af849-3d33-4767-bc6b-58a6045d9c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
